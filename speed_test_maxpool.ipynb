{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good to go!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    import os\n",
    "    drive.mount('/content/drive')\n",
    "    GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'ConvNetFromScratch'\n",
    "    GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
    "    print(os.listdir(GOOGLE_DRIVE_PATH))\n",
    "    import sys\n",
    "    sys.path.append(GOOGLE_DRIVE_PATH)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import time, os, torch, torchvision, random, time, math\n",
    "from torch import Tensor\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from imageio import imread\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (6, 4)\n",
    "plt.rcParams['font.size'] = 10\n",
    "from toolset.utils import *\n",
    "from toolset.data import *\n",
    "from toolset.helper import *\n",
    "from toolset.solver import *\n",
    "from convolutional_networks import *\n",
    "from fully_connected_networks import *\n",
    "from toolset import *\n",
    "from typing import Dict, List, Optional\n",
    "TensorDict = Dict[str, torch.Tensor]\n",
    "if torch.cuda.is_available:\n",
    "    print('Good to go!')\n",
    "else:\n",
    "    print('Please set GPU via Edit -> Notebook Settings.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 25])\n"
     ]
    }
   ],
   "source": [
    "N, C, h, w, ph, pw = 10, 3, 5, 5, 2, 2\n",
    "x = torch.rand(N, C, ph*pw, h*w)\n",
    "idx = torch.max(x, dim = 2).indices\n",
    "print(idx.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape  torch.Size([2, 3, 4, 4])\n",
      "tensor([[[[0.4963, 0.7682, 0.0885, 0.1320],\n",
      "          [0.3074, 0.6341, 0.4901, 0.8964],\n",
      "          [0.4556, 0.6323, 0.3489, 0.4017],\n",
      "          [0.0223, 0.1689, 0.2939, 0.5185]],\n",
      "\n",
      "         [[0.6977, 0.8000, 0.1610, 0.2823],\n",
      "          [0.6816, 0.9152, 0.3971, 0.8742],\n",
      "          [0.4194, 0.5529, 0.9527, 0.0362],\n",
      "          [0.1852, 0.3734, 0.3051, 0.9320]],\n",
      "\n",
      "         [[0.1759, 0.2698, 0.1507, 0.0317],\n",
      "          [0.2081, 0.9298, 0.7231, 0.7423],\n",
      "          [0.5263, 0.2437, 0.5846, 0.0332],\n",
      "          [0.1387, 0.2422, 0.8155, 0.7932]]],\n",
      "\n",
      "\n",
      "        [[[0.2783, 0.4820, 0.8198, 0.9971],\n",
      "          [0.6984, 0.5675, 0.8352, 0.2056],\n",
      "          [0.5932, 0.1123, 0.1535, 0.2417],\n",
      "          [0.7262, 0.7011, 0.2038, 0.6511]],\n",
      "\n",
      "         [[0.7745, 0.4369, 0.5191, 0.6159],\n",
      "          [0.8102, 0.9801, 0.1147, 0.3168],\n",
      "          [0.6965, 0.9143, 0.9351, 0.9412],\n",
      "          [0.5995, 0.0652, 0.5460, 0.1872]],\n",
      "\n",
      "         [[0.0340, 0.9442, 0.8802, 0.0012],\n",
      "          [0.5936, 0.4158, 0.4177, 0.2711],\n",
      "          [0.6923, 0.2038, 0.6833, 0.7529],\n",
      "          [0.8579, 0.6870, 0.0051, 0.1757]]]])\n",
      "\n",
      "torch.Size([2, 3, 4])\n",
      "tensor([[[0, 0, 1, 1],\n",
      "         [0, 1, 2, 3],\n",
      "         [2, 1, 3, 3]],\n",
      "\n",
      "        [[3, 3, 1, 0],\n",
      "         [1, 1, 2, 2],\n",
      "         [3, 0, 0, 2]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 0., 0.],\n",
       "          [0., 0., 1., 1.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[1., 0., 0., 0.],\n",
       "          [0., 1., 0., 0.],\n",
       "          [0., 0., 1., 0.],\n",
       "          [0., 0., 0., 1.]],\n",
       "\n",
       "         [[0., 0., 0., 0.],\n",
       "          [0., 1., 0., 0.],\n",
       "          [1., 0., 0., 0.],\n",
       "          [0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., 1.],\n",
       "          [0., 0., 1., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [1., 1., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0.],\n",
       "          [1., 1., 0., 0.],\n",
       "          [0., 0., 1., 1.],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 1., 1., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 1.],\n",
       "          [1., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_seed(0)\n",
    "N, C, h, w, ph, pw = 2, 3, 2, 2, 2, 2\n",
    "x = torch.rand(N, C, ph*pw, h*w)\n",
    "print(\"x shape \", x.shape)\n",
    "print(x)\n",
    "print()\n",
    "_, idx = torch.max(x, dim=2)  # 使用torch.max返回values和indices\n",
    "print(idx.shape)\n",
    "print(idx)\n",
    "\n",
    "\n",
    "x.zero_().scatter_(2, idx.unsqueeze(2), 1)\n",
    "# output = torch.zeros_like(x)\n",
    "# print(output.shape)\n",
    "# output.scatter_(2, idx.unsqueeze(2), 1)\n",
    "# 观察，最好纸上画个图加深理解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolMy(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(x:Tensor, pool_param):\n",
    "        \"\"\"\n",
    "        最大池化层前向传播的朴素实现。\n",
    "\n",
    "        输入：\n",
    "        - x：输入数据，形状为(N, C, H, W)\n",
    "        - pool_param：字典，包含以下键：\n",
    "        - 'pool_height'：每个池化区域的高度\n",
    "        - 'pool_width'：每个池化区域的宽度\n",
    "        - 'stride'：相邻池化区域之间的距离\n",
    "        这里不需要填充。\n",
    "\n",
    "        返回一个元组：\n",
    "        - out：形状为(N, C, H', W')的输出，其中H'和W'由以下公式给出：\n",
    "        H' = 1 + (H - pool_height) / stride\n",
    "        W' = 1 + (W - pool_width) / stride\n",
    "        - cache：(x, pool_param)\n",
    "        \"\"\"\n",
    "        N, C, H, W = x.shape\n",
    "        ph, pw, stride = pool_param['pool_height'], pool_param['pool_width'], pool_param['stride']\n",
    "        H_out, W_out = 1 + (H - ph) // stride, 1 + (W - pw) // stride\n",
    "        out = torch.zeros(N, C, H_out, W_out, dtype=x.dtype, device=x.device)\n",
    "\n",
    "        x_conv = x.unfold(2, ph, stride).unfold(3, pw, stride)  #@ (N,C,H,W)->(N,C,h,w,ph,pw)\n",
    "        x_colvectorized = x_conv.permute(0, 1, 4, 5, 2, 3).reshape(N, C,ph*pw, -1)   # (N,C,ph,pw,h,w)->(N,C,phpw,hw)\n",
    "        x_col_value, x_col_idx = x_colvectorized.max(dim = 2)  # (N,C,hw)\n",
    "        out = x_col_value.reshape(N, C, H_out, W_out)  # (N,C,hw)->(N,C,h,w)\n",
    "        cache = (x.shape, x_col_idx, x_colvectorized, pool_param)\n",
    "        return out, cache\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(dout:Tensor, cache:Tuple[Tuple,Tensor,Tensor,Dict]):\n",
    "        \"\"\"\n",
    "        最大池化层反向传播的朴素实现。\n",
    "        输入：\n",
    "        - dout：上游导数\n",
    "        - cache：与前向传播中的(x, pool_param)相同的元组。\n",
    "        返回：\n",
    "        - dx：相对于x的梯度\n",
    "        \"\"\"\n",
    "        x_shape, x_col_idx, x_colvectorized, pool_param = cache  # x_col_idx : (N,C,hw)\n",
    "        N, C, H, W = x_shape\n",
    "        ph, pw, stride = pool_param['pool_height'], pool_param['pool_width'], pool_param['stride']\n",
    "        H_out, W_out = 1 + (H - ph) // stride, 1 + (W - pw) // stride\n",
    "        x_colvectorized.zero_().scatter_(2, x_col_idx.unsqueeze(2), 1)  #? why  (N,C,phpw,hw)=(N,C,A,B)\n",
    "        \n",
    "        dout_reshape = dout.reshape(N,C,-1)  # (N,C,B)\n",
    "        tmp = torch.einsum('NCB,NCAB->NCAB',dout_reshape,x_colvectorized).reshape(N, C, ph, pw, H_out, W_out).permute(0, 1, 4, 5, 2, 3)\n",
    "        dx = torch.zeros(size=x_shape,dtype=x_colvectorized.dtype,device=x_colvectorized.device)  # (N,C,H,W)\n",
    "        for top in range(ph):\n",
    "            down = top + H_out * stride\n",
    "            for left in range(pw):\n",
    "                right = left + W_out * stride\n",
    "                dx[:, :, top:down:stride, left:right:stride] += tmp[:, :, :, :, top, left]\n",
    "        return dx\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MaxPool.forward function:\n",
      "difference:  5.921052675939009e-09\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reset_seed(0)\n",
    "x_shape = torch.tensor((2, 3, 4, 4))\n",
    "x = torch.linspace(-0.3, 0.4, steps=torch.prod(x_shape), dtype=torch.float64, device='cuda').reshape(*x_shape)\n",
    "pool_param = {'pool_width': 2, 'pool_height': 2, 'stride': 2}\n",
    "\n",
    "out, _ = MaxPoolMy.forward(x, pool_param)\n",
    "\n",
    "correct_out = torch.tensor([[[[-0.26315789, -0.24842105],\n",
    "                              [-0.20421053, -0.18947368]],\n",
    "                             [[-0.14526316, -0.13052632],\n",
    "                              [-0.08631579, -0.07157895]],\n",
    "                             [[-0.02736842, -0.01263158],\n",
    "                              [ 0.03157895,  0.04631579]]],\n",
    "                            [[[ 0.09052632,  0.10526316],\n",
    "                              [ 0.14947368,  0.16421053]],\n",
    "                             [[ 0.20842105,  0.22315789],\n",
    "                              [ 0.26736842,  0.28210526]],\n",
    "                             [[ 0.32631579,  0.34105263],\n",
    "                              [ 0.38526316,  0.4       ]]]],\n",
    "                           dtype=torch.float64, device='cuda')\n",
    "\n",
    "# Compare your output with ours. Difference should be on the order of e-8.\n",
    "print('Testing MaxPool.forward function:')\n",
    "print('difference: ', grad.rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MaxPool.backward function:\n",
      "dx error:  6.653155794014975e-10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "reset_seed(0)\n",
    "x = torch.randn(3, 2, 8, 8, dtype=torch.float64, device='cuda')\n",
    "dout = torch.randn(3, 2, 4, 4, dtype=torch.float64, device='cuda')\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "dx_num = grad.compute_numeric_gradient(lambda x: MaxPoolMy.forward(x, pool_param)[0], x, dout)\n",
    "\n",
    "out, cache = MaxPoolMy.forward(x, pool_param)\n",
    "dx = MaxPoolMy.backward(dout, cache)\n",
    "\n",
    "print('Testing MaxPool.backward function:')\n",
    "print('dx error: ', grad.rel_error(dx, dx_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'FastConv' from 'convolutional_networks' (D:\\DeepLearning\\ConvNetFromScratch\\convolutional_networks.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Relative errors should be close to 0.0\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconvolutional_networks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv, MaxPool, FastConv, FastMaxPool\n\u001b[0;32m      5\u001b[0m reset_seed(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      6\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat64, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'FastConv' from 'convolutional_networks' (D:\\DeepLearning\\ConvNetFromScratch\\convolutional_networks.py)"
     ]
    }
   ],
   "source": [
    "# Relative errors should be close to 0.0\n",
    "from convolutional_networks import Conv, MaxPool, FastConv, FastMaxPool\n",
    "\n",
    "\n",
    "reset_seed(0)\n",
    "x = torch.randn(40, 3, 32, 32, dtype=torch.float64, device='cuda')\n",
    "dout = torch.randn(40, 3, 16, 16, dtype=torch.float64, device='cuda')\n",
    "x_cuda, dout_cuda = x.to('cuda'), dout.to('cuda')\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "out_naive, cache_naive = MaxPool.forward(x, pool_param)\n",
    "t1 = time.perf_counter()\n",
    "out_fast, cache_fast = FastMaxPool.forward(x, pool_param)\n",
    "t2 = time.perf_counter()\n",
    "out_fast_cuda, cache_fast_cuda = FastMaxPool.forward(x_cuda, pool_param)\n",
    "t3 = time.perf_counter()\n",
    "out_my, cache_my = MaxPoolMy.forward(x, pool_param)\n",
    "t4 = time.perf_counter()\n",
    "\n",
    "print('Testing FastMaxPool.forward:')\n",
    "print('Naive: %fs' % (t1 - t0))\n",
    "print('Fast: %fs' % (t2 - t1))\n",
    "print('Fast CUDA: %fs' % (t3 - t2))\n",
    "print('My: %fs' % (t4 - t3))\n",
    "print('Speedup: %fx' % ((t1 - t0) / (t2 - t1)))\n",
    "print('Speedup CUDA: %fx' % ((t1 - t0) / (t3 - t2)))\n",
    "print('Difference: ', grad.rel_error(out_naive, out_fast))\n",
    "print('Difference CUDA: ', grad.rel_error(out_naive, out_fast_cuda.to(out_naive.device)))\n",
    "print('Difference My: ', grad.rel_error(out_my, out_my.to(out_naive.device)))\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "dx_naive = MaxPool.backward(dout, cache_naive)\n",
    "t1 = time.perf_counter()\n",
    "dx_fast = FastMaxPool.backward(dout, cache_fast)\n",
    "t2 = time.perf_counter()\n",
    "dx_fast_cuda = FastMaxPool.backward(dout_cuda, cache_fast_cuda)\n",
    "t3 = time.perf_counter()\n",
    "dx_my = MaxPoolMy.backward(dout_cuda, cache_my)\n",
    "t4 = time.perf_counter()\n",
    "\n",
    "print('\\nTesting FastMaxPool.backward:')\n",
    "print('Naive: %fs' % (t1 - t0))\n",
    "print('Fast: %fs' % (t2 - t1))\n",
    "print('Fast CUDA: %fs' % (t3 - t2))\n",
    "print('My: %fs' % (t4 - t3))\n",
    "print('Speedup: %fx' % ((t1 - t0) / (t2 - t1)))\n",
    "print('Speedup CUDA: %fx' % ((t1 - t0) / (t3 - t2)))\n",
    "print('dx difference: ', grad.rel_error(dx_naive, dx_fast))\n",
    "print('dx difference CUDA: ', grad.rel_error(dx_naive, dx_fast_cuda.to(dx_naive.device)))\n",
    "print('dx difference My: ', grad.rel_error(dx_naive, dx_my.to(dx_naive.device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
