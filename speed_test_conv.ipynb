{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Good to go!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "try:\n",
    "    import os\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'ConvNetFromScratch' \n",
    "    GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
    "    print(os.listdir(GOOGLE_DRIVE_PATH))\n",
    "    import sys\n",
    "    sys.path.append(GOOGLE_DRIVE_PATH)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import time, os, torch, torchvision, random, time, math\n",
    "from torch import Tensor\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from imageio import imread\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (6, 4)\n",
    "plt.rcParams['font.size'] = 10\n",
    "from toolset.utils import *\n",
    "from toolset.data import *\n",
    "from toolset.helper import *\n",
    "from toolset.solver import *\n",
    "from convolutional_networks import *\n",
    "from fully_connected_networks import *\n",
    "from toolset import *\n",
    "from typing import Dict, List, Optional\n",
    "TensorDict = Dict[str, torch.Tensor]\n",
    "if torch.cuda.is_available:\n",
    "    print('Good to go!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Conv.forward:\n",
      "Vanilla: 4.527313s\n",
      "Fast:0.003057s\n",
      "Fast CUDA:0.000476s\n",
      "Speedup: 1480.9x\n",
      "Speedup CUDA: 9501.2x\n",
      "Difference:  2.668446055776658e-16\n",
      "Difference CUDA:  2.668446055776658e-16\n",
      "\n",
      "Testing FastConv.backward:\n",
      "Vanilla: 9.240463s\n",
      "Fast:0.006705s\n",
      "Fast CUDA:0.000537s\n",
      "Speedup: 1378.1x\n",
      "Speedup CUDA: 17217.2x\n",
      "dx difference:  3.411315230434022e-16\n",
      "dw difference:  2.524463004486185e-15\n",
      "db difference:  0.0\n",
      "dx difference CUDA:  3.411315230434022e-16\n",
      "dw difference CUDA:  2.5645338458272362e-15\n",
      "db difference CUDA:  1.8505571847128472e-16\n"
     ]
    }
   ],
   "source": [
    "# 用torch.float64/double Rel errors 应该在1e-11附近或更小\n",
    "# time.time时间太短精度不够会造成除0问题，用time.perf_counter()\n",
    "from time import perf_counter as tpc\n",
    "reset_seed(0)\n",
    "\n",
    "\n",
    "N, C, H, W = 50, 3, 32, 32\n",
    "F, HH, WW = 25, 3, 3\n",
    "stride, pad = 2, 1\n",
    "h_out, w_out = 1 + (H + 2 * pad - HH) // stride, 1 + (W + 2 * pad - WW) // stride\n",
    "\n",
    "conv_param = {'stride': stride, 'pad': pad}\n",
    "dtype = torch.float64\n",
    "device = 'cpu'\n",
    "\n",
    "x = torch.randn(N, C, H, W, dtype=dtype, device=device)\n",
    "w = torch.randn(F, C, HH, WW, dtype=dtype, device=device)\n",
    "b = torch.randn(F, dtype=dtype, device=device)\n",
    "dout = torch.randn(N, F, h_out, w_out, dtype=dtype, device=device)\n",
    "x_cuda, w_cuda, b_cuda, dout_cuda = x.to('cuda'), w.to('cuda'), b.to('cuda'), dout.to('cuda')\n",
    "\n",
    "# gpu需要预热\n",
    "_, _ = Conv.forward(x_cuda, w_cuda, b_cuda, conv_param)\n",
    "\n",
    "t0 = tpc()\n",
    "out_vanilla, cache_vanilla = ConvVanilla.forward(x, w, b, conv_param)\n",
    "t1 = tpc()\n",
    "out_fast, cache_fast = Conv.forward(x, w, b, conv_param)\n",
    "t2 = tpc()\n",
    "out_fast_cuda, cache_fast_cuda = Conv.forward(x_cuda, w_cuda, b_cuda, conv_param)\n",
    "t3 = tpc()\n",
    "\n",
    "print('Testing Conv.forward:')\n",
    "print(f'Vanilla: {t1 - t0:f}s')\n",
    "print(f'Fast:{t2 - t1:f}s')\n",
    "print(f'Fast CUDA:{t3 - t2:f}s')\n",
    "print(f'Speedup: {(t1 - t0) / (t2 - t1):.1f}x')\n",
    "print(f'Speedup CUDA: {(t1 - t0) / (t3 - t2):.1f}x')\n",
    "print('Difference: ', grad.rel_error(out_vanilla, out_fast))\n",
    "print('Difference CUDA: ', grad.rel_error(out_vanilla, out_fast_cuda.to(device)))\n",
    "\n",
    "t0 = tpc()\n",
    "dx_vanilla, dw_vanilla, db_vanilla = ConvVanilla.backward(dout, cache_vanilla)\n",
    "t1 = tpc()\n",
    "dx_fast, dw_fast, db_fast = Conv.backward(dout, cache_fast)\n",
    "t2 = tpc()\n",
    "dx_fast_cuda, dw_fast_cuda, db_fast_cuda = Conv.backward(dout_cuda, cache_fast_cuda)\n",
    "t3 = tpc()\n",
    "\n",
    "print('\\nTesting FastConv.backward:')\n",
    "print(f'Vanilla: {t1 - t0:f}s')\n",
    "print(f'Fast:{t2 - t1:f}s')\n",
    "print(f'Fast CUDA:{t3 - t2:f}s')\n",
    "print(f'Speedup: {(t1 - t0) / (t2 - t1):.1f}x')\n",
    "print(f'Speedup CUDA: {(t1 - t0) / (t3 - t2):.1f}x')\n",
    "\n",
    "print('dx difference: ', grad.rel_error(dx_vanilla, dx_fast))\n",
    "print('dw difference: ', grad.rel_error(dw_vanilla, dw_fast))\n",
    "print('db difference: ', grad.rel_error(db_vanilla, db_fast))\n",
    "\n",
    "print('dx difference CUDA: ', grad.rel_error(dx_vanilla, dx_fast_cuda.to(dx_vanilla.device)))\n",
    "print('dw difference CUDA: ', grad.rel_error(dw_vanilla, dw_fast_cuda.to(dw_vanilla.device)))\n",
    "print('db difference CUDA: ', grad.rel_error(db_vanilla, db_fast_cuda.to(db_vanilla.device)))\n",
    "# 至少1000倍以上的加速"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 其他"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. unfold()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.Tensor.unfold()`是一个函数，用于创建一个视图，其中沿着指定维度展开的数据被组织成一个新的最后一维。换句话说，这个操作可以用于有效地提取张量的滑动窗口块，用于进一步的操作（如卷积）。\n",
    "\n",
    "这个函数的语法是这样的：\n",
    "\n",
    "`tensor.unfold(dimension, size, step)`\n",
    "\n",
    "- `dimension` 是你想要展开的维度\n",
    "- `size` 是滑动窗口的大小\n",
    "- `step` 是滑动窗口移动的步长\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [2, 3, 4],\n",
      "        [3, 4, 5],\n",
      "        [4, 5, 6],\n",
      "        [5, 6, 7]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 8)  # tensor([1, 2, 3, 4, 5, 6, 7])\n",
    "y = x.unfold(0, 3, 1)  # 连续的滑动窗口\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11],\n",
      "        [12, 13, 14, 15, 16, 17],\n",
      "        [18, 19, 20, 21, 22, 23],\n",
      "        [24, 25, 26, 27, 28, 29]])\n",
      "2 3\n",
      "torch.Size([2, 3, 2, 2])\n",
      "tensor([[[[ 0,  1],\n",
      "          [ 6,  7]],\n",
      "\n",
      "         [[ 2,  3],\n",
      "          [ 8,  9]],\n",
      "\n",
      "         [[ 4,  5],\n",
      "          [10, 11]]],\n",
      "\n",
      "\n",
      "        [[[12, 13],\n",
      "          [18, 19]],\n",
      "\n",
      "         [[14, 15],\n",
      "          [20, 21]],\n",
      "\n",
      "         [[16, 17],\n",
      "          [22, 23]]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(30).view(5, 6)\n",
    "print(x)\n",
    "H, W = x.shape\n",
    "HH = WW = 2\n",
    "pad = 0\n",
    "stride = 2\n",
    "H_out = 1 + (H + 2 * pad - HH) // stride\n",
    "W_out = 1 + (W + 2 * pad - WW) // stride\n",
    "\n",
    "print(H_out,W_out)\n",
    "\n",
    "def get_conv_table(x, HH, WW, stride):\n",
    "    #@ 针对按顺序的两维(这里是0，1)逐个unfold就可以得到要卷积的部分\n",
    "    y = x.unfold(0, HH, stride).unfold(1, WW, stride)\n",
    "    return y\n",
    "y = get_conv_table(x, HH,WW,stride)\n",
    "#print(y)\n",
    "print(y.shape)\n",
    "print(y)\n",
    "# 输出：\n",
    "# tensor([[[[1, 2],\n",
    "#            [4, 5]],\n",
    "#           [[2, 3],\n",
    "#            [5, 6]]],\n",
    "#          [[[4, 5],\n",
    "#            [7, 8]],\n",
    "#           [[5, 6],\n",
    "#            [8, 9]]]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. einsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n"
     ]
    }
   ],
   "source": [
    "# 转置\n",
    "x = torch.arange(6).reshape((2,3))\n",
    "print(x)\n",
    "# tensor([[0, 1, 2],\n",
    "#         [3, 4, 5]])\n",
    "\n",
    "# 使用einsum进行转置操作\n",
    "y = torch.einsum('ij->ji', x)\n",
    "print(y)\n",
    "# tensor([[0, 3],\n",
    "#         [1, 4],\n",
    "#         [2, 5]])\n",
    "\n",
    "# 最后两维转置\n",
    "a = torch.randn(2,3,5,7,9)\n",
    "# i = 7, j = 9\n",
    "b = torch.einsum('...ij->...ji', [a])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15, 18, 21],\n",
      "        [42, 54, 66]])\n"
     ]
    }
   ],
   "source": [
    "# 矩阵乘法\n",
    "x = torch.arange(6).reshape((2,3))\n",
    "y = torch.arange(9).reshape((3,3))\n",
    "\n",
    "# 使用einsum进行矩阵乘法操作\n",
    "z = torch.einsum('ij,jk->ik', x, y)\n",
    "print(z)\n",
    "# tensor([[15, 18, 21],\n",
    "#         [42, 54, 66]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2]) tensor([3, 4, 5])\n",
      "tensor([ 0,  4, 10])\n",
      "tensor(14)\n"
     ]
    }
   ],
   "source": [
    "# 张量点乘求和\n",
    "x = torch.arange(3)\n",
    "y = torch.arange(3, 6)\n",
    "print(x, y)\n",
    "# 使用einsum进行点乘操作\n",
    "z = torch.einsum('i,i->i', x, y)\n",
    "print(z)\n",
    "# tensor([ 0,  4, 10])\n",
    "\n",
    "# 使用einsum进行点乘+求和操作\n",
    "z = torch.einsum('i,i->', x, y)\n",
    "print(z)\n",
    "# tensor(14)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2,  3,  4,  5],\n",
      "         [ 6,  7,  8,  9, 10, 11],\n",
      "         [12, 13, 14, 15, 16, 17],\n",
      "         [18, 19, 20, 21, 22, 23]],\n",
      "\n",
      "        [[24, 25, 26, 27, 28, 29],\n",
      "         [30, 31, 32, 33, 34, 35],\n",
      "         [36, 37, 38, 39, 40, 41],\n",
      "         [42, 43, 44, 45, 46, 47]]])\n",
      "2 3\n",
      "torch.Size([2, 2, 3, 2, 2])\n",
      "tensor([[[[[ 0,  1],\n",
      "           [ 6,  7]],\n",
      "\n",
      "          [[ 2,  3],\n",
      "           [ 8,  9]],\n",
      "\n",
      "          [[ 4,  5],\n",
      "           [10, 11]]],\n",
      "\n",
      "\n",
      "         [[[12, 13],\n",
      "           [18, 19]],\n",
      "\n",
      "          [[14, 15],\n",
      "           [20, 21]],\n",
      "\n",
      "          [[16, 17],\n",
      "           [22, 23]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[24, 25],\n",
      "           [30, 31]],\n",
      "\n",
      "          [[26, 27],\n",
      "           [32, 33]],\n",
      "\n",
      "          [[28, 29],\n",
      "           [34, 35]]],\n",
      "\n",
      "\n",
      "         [[[36, 37],\n",
      "           [42, 43]],\n",
      "\n",
      "          [[38, 39],\n",
      "           [44, 45]],\n",
      "\n",
      "          [[40, 41],\n",
      "           [46, 47]]]]])\n",
      "torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(2*24).view(2, 4, 6)  # 理解成双通道图像\n",
    "print(x)\n",
    "C, H, W = x.shape\n",
    "HH = WW = 2\n",
    "pad = 0\n",
    "stride = 2\n",
    "H_out = 1 + (H + 2 * pad - HH) // stride\n",
    "W_out = 1 + (W + 2 * pad - WW) // stride\n",
    "\n",
    "print(H_out,W_out)\n",
    "\n",
    "def get_conv_table(x, HH, WW, stride):\n",
    "    #@ 针对按顺序的两维(这里是1，2)逐个unfold就可以得到要卷积的部分\n",
    "    y = x.unfold(1, HH, stride).unfold(2, WW, stride)\n",
    "    return y\n",
    "y = get_conv_table(x, HH,WW,stride)\n",
    "#print(y)\n",
    "print(y.shape)  # (N, H_out, W_out, stride, stride)\n",
    "print(y)\n",
    "\n",
    "# 对角取元素\n",
    "# 双通道图像需要双通道卷积核(单个)\n",
    "w = torch.tensor([\n",
    "    [[1, 0],\n",
    "     [0, 1]],\n",
    "    [[1, 0],\n",
    "     [0, 1]]\n",
    "    \n",
    "])\n",
    "print(w.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "假设输入张量 `x` 的形状为 `(C, H, W)`，即通道数为 `C`，高度为 `H`，宽度为 `W`。卷积核 `w` 的形状为 `(C, HH, WW)`，即通道数也为 `C`，高度为 `HH`，宽度为 `WW`。此处的 `y` 是 `x` 通过 `unfold` 操作后的结果，形状为 `(C, H_out, W_out, HH, WW)`，其中 `H_out` 和 `W_out` 分别是输出的高度和宽度。\n",
    "\n",
    "下面的 `einsum` 表达式 `'ChwIJ,CIJ->hw'` 可以表示为以下数学公式：\n",
    "\n",
    "$$out_{hw}=\\sum_{C,\\text{堆叠相加}}{\\sum_{I=0}^{HH-1}{\\sum_{J=0}^{WW-1}{y_{ChwIJ}\\cdot}}}w_{CIJ}$$\n",
    "\n",
    "其中，`h` 和 `w` 是输出张量的高度和宽度的索引，`C` 是通道的索引，`I` 和 `J` 是卷积核的高度和宽度的索引。这个表达式的含义是，对于输出张量的每一个位置 `(h, w)`，我们遍历输入的所有通道和卷积核的所有位置，将输入的对应部分和卷积核的元素相乘，然后将所有结果相加，得到输出张量的该位置的值。\n",
    "\n",
    "这就是使用 `einsum` 进行卷积运算的数学公式表示。这个表达式就是卷积运算的定义：将输入的每一个局部窗口和卷积核进行对应元素的乘积和操作，得到输出的对应位置的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 62,  70,  78],\n",
      "        [110, 118, 126]])\n"
     ]
    }
   ],
   "source": [
    "# h, w是输出维度，I，J是卷积核大小\n",
    "out = torch.einsum('ChwIJ,CIJ->hw', y, w)\n",
    "print(out)\n",
    "# tensor([[ 0+7+24+31,2+9+26+33,  78], \n",
    "#         [110, 118, 126]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "关于 `'NChwIJ,FCIJ->NFhw'` 这个表达式的数学公式如下：\n",
    "$$\n",
    "\\text{out}_{NFhw} = \\sum_{C} \\sum_{I=0}^{HH-1} \\sum_{J=0}^{WW-1} x_{NChwIJ} \\cdot w_{FCIJ}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# N, C, h, w, I, J = 2, 3, 2, 4, 3, 3\n",
    "# C, H, W = 3, 4, 6\n",
    "# x = torch.ones(N, C, H, W)\n",
    "# # print(x)\n",
    "# col = ConvMy.im2col(x, I, J)  \n",
    "# print(col.shape) # (N, h*w, C*I*J)\n",
    "# out = ConvMy.col2im(col, x.shape,I, J)\n",
    "# print(out.shape)\n",
    "# print(x, out, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# N, C, h, w, I, J = 2, 3, 2, 4, 3, 3\n",
    "# C, H, W = 3, 4, 6\n",
    "# F = 2\n",
    "# x1 = torch.zeros(N, h, w, C, I, J)\n",
    "# # print(x.shape)\n",
    "# weight = torch.randint(0,10,[F, C, I, J])\n",
    "# w_expand1 = weight[:,None,None,:,:,:]  # (F11CIJ)\n",
    "# print(w_expand1.shape)\n",
    "# for i in range(w_expand1.shape[0]):\n",
    "#     x1 += w_expand1[i] #  (N, h, w, C, I, J) + (1, 1, 1, C, I, J)\n",
    "# # print(x)\n",
    "\n",
    "# x2 = torch.zeros(N, C, h, w, I, J)\n",
    "# w_expand2 = weight[:, :, None, None, :, :] # (FC11IJ)\n",
    "# print(w_expand2.shape)\n",
    "\n",
    "# for i in range(w_expand2.shape[0]):\n",
    "#     x2 += w_expand2[i] #  (N, C, h, w, I, J) + (1, C, 1, 1, I, J)\n",
    "# x1 = x1.permute(0,3,1,2,4,5)\n",
    "# print(x1 == x2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
